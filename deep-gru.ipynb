{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "QdVlocBYXYba"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers=1):\n",
    "        super(RNN, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.decoder = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, input, hidden):\n",
    "        # input = self.encoder(input)\n",
    "        output, hidden = self.gru(input, hidden)\n",
    "        output = self.decoder(output)\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        if batch_size == 0:\n",
    "            return Variable(torch.zeros(self.num_layers, self.hidden_size))\n",
    "        else:   \n",
    "            return Variable(torch.zeros(self.num_layers, batch_size, self.hidden_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "7FSWbxWCXn9S"
   },
   "outputs": [],
   "source": [
    "chunk_len = 200\n",
    "\n",
    "def train(inp, target):\n",
    "    hidden = decoder.init_hidden(inp.size(0))\n",
    "    decoder.zero_grad()\n",
    "\n",
    "    logits, hidden = decoder(inp, hidden)\n",
    "    loss = criterion(logits.permute(0, 2, 1), target.permute(0, 2, 1))\n",
    "\n",
    "    loss.backward()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "96r0l4t-X73k"
   },
   "outputs": [],
   "source": [
    "def get_vocabulary_and_mappings (chars):\n",
    "    vocabulary = list(set(chars))\n",
    "    vocabulary.sort()\n",
    "    print(\"Vocabulary size: \", len(vocabulary))\n",
    "    \n",
    "    index_to_char = {}\n",
    "    char_to_index = {}\n",
    "    \n",
    "    for idx, char in enumerate(vocabulary):\n",
    "        index_to_char[idx] = char\n",
    "        char_to_index[char] = idx\n",
    "    \n",
    "    return vocabulary, index_to_char, char_to_index\n",
    "\n",
    "def get_batch(chars, char_to_index, V, sequence_size, batch_size):\n",
    "    x = torch.empty((batch_size, sequence_size, V))\n",
    "    y = torch.empty((batch_size, sequence_size, V))\n",
    "    \n",
    "    chars = torch.tensor([char_to_index[char] for char in chars])\n",
    "\n",
    "    batch_id = 0\n",
    "    sequence_id = 0\n",
    "    num_chars = len(chars)\n",
    "    num_sequences = (num_chars-1) // sequence_size\n",
    "    \n",
    "    for i in range(num_sequences):\n",
    "        start_idx = i * sequence_size\n",
    "        end_idx = start_idx + sequence_size\n",
    "        \n",
    "        chars_in = chars[start_idx : end_idx]\n",
    "        chars_out = chars[start_idx + 1 : end_idx + 1]\n",
    "        \n",
    "        x[batch_id] = nn.functional.one_hot(chars_in, V)\n",
    "        y[batch_id] = nn.functional.one_hot(chars_out, V)\n",
    "        \n",
    "        batch_id += 1\n",
    "        \n",
    "        if batch_id == batch_size:\n",
    "            batch_id = 0\n",
    "            yield x, y\n",
    "    \n",
    "    last_batch_size = num_sequences % batch_size\n",
    "    if last_batch_size != 0:\n",
    "        yield x[:last_batch_size], y[:last_batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MVH95x5sX87h",
    "outputId": "8708b8b0-b1df-4303-bad6-bc51a04c816b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size:  60\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "corpus = open('input.txt', 'r').read()\n",
    "chars = [char for char in corpus if char.isalpha() or char in string.punctuation or char =='\\n' or char == ' ']\n",
    "vocabulary, index_to_char, char_to_index = get_vocabulary_and_mappings(chars)\n",
    "V = len(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "v4rXh628apdM"
   },
   "outputs": [],
   "source": [
    "def evaluate(prime_str='A', predict_len=100, temperature=0.8):\n",
    "    hidden = decoder.init_hidden(0)  # batch size is 1\n",
    "    char_x = torch.tensor(char_to_index[prime_str])\n",
    "    x = nn.functional.one_hot(char_x, V).reshape(1, V).to(torch.float32)\n",
    "    predicted = prime_str\n",
    "\n",
    "    for p in range(predict_len):\n",
    "        output, hidden = decoder(x, hidden)\n",
    "\n",
    "        # Sample from the network as a multinomial distribution\n",
    "        output_dist = output.data.view(-1).div(temperature).exp()\n",
    "        top_i = torch.multinomial(output_dist, 1)[0]\n",
    "\n",
    "        # Add predicted character to string and use as next input\n",
    "        predicted_char = index_to_char[top_i.item()]\n",
    "        predicted += predicted_char\n",
    "        x = nn.functional.one_hot(top_i, V).reshape(1, V).to(torch.float32)\n",
    "\n",
    "    return predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "8A1apRzJapOE"
   },
   "outputs": [],
   "source": [
    "import time, math\n",
    "\n",
    "def time_since(since):\n",
    "    s = time.time() - since\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AAMPh-IKYKGz",
    "outputId": "d7e06c16-3df4-417d-ceb0-5f5c9e5f2256"
   },
   "outputs": [],
   "source": [
    "n_epochs = 2000\n",
    "print_every = 10\n",
    "plot_every = 200\n",
    "hidden_size = 512\n",
    "n_layers = 1\n",
    "lr = 0.005\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "decoder = RNN(V, hidden_size, V, n_layers)\n",
    "decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "start = time.time()\n",
    "all_losses = []\n",
    "loss_avg = 0\n",
    "\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    itr = 1\n",
    "    for x, y_true in get_batch(chars, char_to_index, V, chunk_len, batch_size):\n",
    "        loss = train(x, y_true)       \n",
    "        loss_avg += loss\n",
    "\n",
    "        if itr % plot_every == 0:\n",
    "            loss = loss_avg / plot_every\n",
    "            all_losses.append(loss)\n",
    "            loss_avg = 0\n",
    "            print(f\"Iteration: {itr} Loss: {loss:.6f}\")\n",
    "    \n",
    "        itr += 1\n",
    "        \n",
    "    if epoch % print_every == 0:\n",
    "        print(\"\\n--- Generating sample ---\\n\")\n",
    "        print('[%s (%d %d%%) %.4f]' % (time_since(start), epoch, epoch / n_epochs * 100, loss))\n",
    "        char = index_to_char[np.random.randint(V)]\n",
    "        print(evaluate(char, 250))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sometime on the people.\n",
      "\n",
      "SICINIUS:\n",
      "The discorth as the world.\n",
      "\n",
      "CORIOLANUS:\n",
      "What we call'd the rest of him off that\n",
      "The gods and powers at the world,\n",
      "How did see the rather proud to the people along with me.\n",
      "\n",
      "SICINIUS:\n",
      "Speak to your honour'd and the banishmen.\n",
      "\n",
      "SICINIUS:\n",
      "And thousand to be your voices, by the world.\n",
      "\n",
      "CORIOLANUS:\n",
      "He would down would ye the people.\n",
      "\n",
      "SICINIUS:\n",
      "You shall begg'd. What o' the world\n",
      "In a grown for the consent of the body.\n",
      "\n",
      "SICINIUS:\n",
      "He last it is it am as a man down.\n",
      "\n",
      "SICINIUS:\n",
      "Call place, what I then on the tongues.\n",
      "\n",
      "SICINIUS:\n",
      "Hence may be do not be to strange trage.\n",
      "\n",
      "BRUTUS:\n",
      "We'll the people, while that I pray them.\n",
      "\n",
      "SICINIUS:\n",
      "What o' the contribless of the people,\n",
      "He was a day to stone of my chair.\n",
      "\n",
      "SICINIUS:\n",
      "First to be content to the people.\n",
      "\n",
      "CORIOLANUS:\n",
      "Why, no, no, and to the denited.\n",
      "\n",
      "SICINIUS:\n",
      "This way and the tongue on the dare to the people,\n",
      "And to the tongues and way, or belly that he done.\n",
      "\n",
      "BRUTUS:\n",
      "The people, being the gods on\n",
      "To be you to the gra\n"
     ]
    }
   ],
   "source": [
    "print(evaluate('S', 1000, 0.5))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
